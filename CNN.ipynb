{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnDOkEi1k1qX"
      },
      "source": [
        "library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKaerybY0XsR"
      },
      "outputs": [],
      "source": [
        "!pip install contractions\n",
        "!pip install txtsearch\n",
        "!pip install tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbnS7pdtoeQr"
      },
      "source": [
        "Jalankan Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u-9v_ytks3u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPoling1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# panjang seed\n",
        "seed = 42\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10o70uS5qVQl",
        "outputId": "11203f9a-f010-4620-f9ab-d7fa6b0db8ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08rb3MSVqchn"
      },
      "outputs": [],
      "source": [
        "# KeyedVector berguna untuk menampung vektor siap pakai berukuran kecil dari word embedding model\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJjfJy9x3B3s"
      },
      "source": [
        "Dibawah ini Perhatikan tempan menaruh file **vectornya**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uygwfPaxqlRb",
        "outputId": "9b7edd20-ba9b-4f19-b156-e65df32b5ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waktu proses:  80.15471482276917\n"
          ]
        }
      ],
      "source": [
        "# URL file vektor\n",
        "url_vektor = \"https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.id.vec\"\n",
        "\n",
        "# Lokasi tempat Anda ingin menyimpan file vektor\n",
        "lokasi_simpan = '/content/drive/MyDrive/wiki.id.vec'\n",
        "\n",
        "waktu_mulai = time.time()\n",
        "\n",
        "# Unduh file vektor dari URL\n",
        "response = requests.get(url_vektor)\n",
        "with open(lokasi_simpan, 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# Load vektor dari kata\n",
        "vektor_dari_kata = KeyedVectors.load_word2vec_format(lokasi_simpan, limit=0)\n",
        "\n",
        "waktu_selesai = time.time()\n",
        "waktu_proses = waktu_selesai - waktu_mulai\n",
        "print(\"Waktu proses: \", waktu_proses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh7QYmT7ouL3"
      },
      "source": [
        "#Inputan Vektor Embedding & perhatikan tempat menaruh kosakata.text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejjdEoc9o3_u"
      },
      "outputs": [],
      "source": [
        "import r\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from difflib import SequenceMatcher\n",
        "from collections import Counter\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "pd.set_option('mode.chained_assignment', None)\n",
        "input_tet = 'kang berapa harga daypack?'\n",
        "# Hapus simbol\n",
        "hapus_simbol = re.sub(r'[^\\w]', ' ', input_text)\n",
        "#penyesuaian kata\n",
        "kalimat_koreksi = hapus_simbol.lower()\n",
        "\n",
        "# Read words from the text file\n",
        "with open('/content/drive/MyDrive/namafolder/katadasar.txt', 'r') as file:\n",
        "    words_list = file.read().split()\n",
        "\n",
        "# Create a frequency counter for the words\n",
        "WORDS = Counter(words_list)\n",
        "\n",
        "def P(word, N=sum(WORDS.values())):\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word):\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word):\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words):\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [L + R[1:] for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "    inserts = [L + c + R for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "kata_koreksi = []\n",
        "for kata in kalimat_koreksi.split():\n",
        "    kata_koreksi.append(correction(kata))\n",
        "\n",
        "hasil_koreksi = ' '.join(kata_koreksi)\n",
        "\n",
        "print('Input Kalimat:', input_text)\n",
        "print('Hasil Koreksi:', hasil_koreksi)\n",
        "\n",
        "# komputasi untuk mendapatkan vektor dari setiap kata\n",
        "for kata in hasil_koreksi.split():\n",
        "    if kata in vektor_dari_kata:\n",
        "        word_embedding_kata = vektor_dari_kata[kata]\n",
        "        print(word_embedding_kata)\n",
        "\n",
        "# Similarity\n",
        "data['skor'] = data['question'].apply(lambda row: SequenceMatcher(None, hasil_koreksi, row).ratio())\n",
        "\n",
        "# Find the index of the most similar question\n",
        "most_similar_idx = data['skor'].idxmax()\n",
        "\n",
        "# Extract the predicted tag based on the most similar question\n",
        "predicted_tag = data.loc[most_similar_idx, 'label']\n",
        "\n",
        "# Filter responses based on the predicted tag\n",
        "response = data[data['label'] == predicted_tag]\n",
        "\n",
        "# Find the response with the highest similarity score\n",
        "tes = response.loc[response['skor'].idxmax()]\n",
        "\n",
        "print(\"Predicted Tag:\", predicted_tag)\n",
        "print(\"Predicted Answer:\", tes.answer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUP_m_H-o6XB"
      },
      "source": [
        "#Perhatikan tempat menyimpan dataset untuk dibaca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyUwzJHVpFEA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_excel('/content/drive/MyDrive/namafolder/dataset_super_final.xltx')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owWR98kyvPoe"
      },
      "outputs": [],
      "source": [
        "data['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3u_Hz5HvBb8"
      },
      "outputs": [],
      "source": [
        "encoder= LabelEncoder()\n",
        "encoder.fit(data['label'])\n",
        "training_labels = encoder.transform(data['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5LxeIS5pF_m"
      },
      "source": [
        "#Preprocessing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVs_H1SypOGK"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize  # Import the word_tokenize function\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Connect to Google Drive if you are using Google Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Path to the Excel file on Google Drive\n",
        "file_path = '/content/drive/MyDrive/1167050002/dataset_super_final.xltx'\n",
        "\n",
        "# Read data from the Excel file\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Name of the column containing the questions to preprocess and tokenize\n",
        "target_column = 'questions'\n",
        "\n",
        "# Function to preprocess text (remove symbols and lowercase)\n",
        "def preprocess_text(text):\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)  # Remove symbols\n",
        "    folded_text = cleaned_text.lower()  # Convert to lowercase\n",
        "    return folded_text\n",
        "\n",
        "# Function to tokenize text using NLTK's word_tokenize\n",
        "def tkenize_text(text):\n",
        "    tokens = word_tokenize(text)  # Tokenize words\n",
        "    return tokens\n",
        "\n",
        "# Preprocessing (cleaning and case-folding) on the 'question' column\n",
        "data['clened_question'] = data[target_column].apply(preprocess_text)\n",
        "\n",
        "# Tokenization on the preprocessed column\n",
        "data['tokeized_question'] = data['cleaned_question'].apply(tokenize_text)\n",
        "\n",
        "# Display preprocessing results (cleaned_question)\n",
        "print(\"Output Pertama: Hasil Cleaning dan Case-Folding\")\n",
        "print(data[['question', 'cleaned_question']])\n",
        "\n",
        "# Display tokenization results (tokenized_question)\n",
        "print(\"\\nOutput Kedua: Hasil Tokenisasi\")\n",
        "print(data[['cleaned_question', 'tokenized_question']])\n",
        "\n",
        "# Tokenization and sequence encoding configuration\n",
        "vocab_size = 1000\n",
        "embedding_dim = 16\n",
        "max_len = 20\n",
        "oov_token = \"<OOV>\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(data[target_column])\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(data[target_column])\n",
        "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)\n",
        "\n",
        "# print(\"\\nHasil Tokenisasi dan Pengkodean Urutan:\")\n",
        "# print(padded_sequences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqApnU9cpRu7"
      },
      "source": [
        "# Model CNN & Pelatihan\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysAHqLN3pXI1"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "from wandb.keras import WandbCallback\n",
        "import wandb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, BatchNormalization, MaxPooling1D, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# Langkah 1: Inisialisasi WandB run\n",
        "wandb.init(project='cnn_azis')\n",
        "\n",
        "# Tentukan konstanta\n",
        "vocab_size = 10000  # Ganti dengan ukuran vocabulary Anda\n",
        "EMBED_SIZE = 300\n",
        "MAX_SEQUENCE_LENGTH = 20\n",
        "\n",
        "# Langkah 2: Simpan input model dan hiperparameter\n",
        "config = wandb.config\n",
        "config.learning_rate = 0.001\n",
        "\n",
        "# Buat model Anda\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, EMBED_SIZE, input_length=MAX_SEQUENCE_LENGTH))\n",
        "model.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())  # Import BatchNormalization from keras.layers\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "model.add(BatchNrmalization())  # Import BatchNormalization from keras.layers\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "model.add(BatchNormaliation())  # Import BatchNormalization from keras.layers\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# latih model Pembelajaran CNN\n",
        "def lr_schdule(epoch):\n",
        "    initial_learning_rate = 0.001\n",
        "    decay_steps = 10000\n",
        "    decay_rate = 0.9\n",
        "    if epoch < 50:\n",
        "        return initial_learning_rate\n",
        "    else:\n",
        "        return initial_learning_rate * decay_rate ** (epoch // decay_steps)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# Kompilasi model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(loss='binary_crossentropy', opimizer=opimizer, metrics=['accuracy'])\n",
        "\n",
        "# Cetak ringkasan model\n",
        "model.summary()\n",
        "\n",
        "num_samples = 1000\n",
        "vocab_size = 1000\n",
        "MAX_SEQUENCE_LENGTH = 20\n",
        "\n",
        "# sumbu x dan y\n",
        "import numpy as np\n",
        "X = np.random.randint(0, vocb_size, size=(1000, MAX_SEQUENCE_LENGTH))\n",
        "y = np.random.randint(0, 2, size=(200,))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU-vQUzqpYCo"
      },
      "source": [
        "# Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKyr_eCKpjXT"
      },
      "outputs": [],
      "source": [
        "# Langkah 3: Pelatihan model dengan WandB Callback\n",
        "epochs = 90\n",
        "history = model.fit(padded_sequences, np.array(training_labels), epoch=epochs, callbacks=[WandbCallback()])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
